{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree in MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from utils import show_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST('datasets/mnist')\n",
    "train_images, train_labels = mnist.load_training()\n",
    "test_images, test_labels = mnist.load_testing()\n",
    "\n",
    "shape = (28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Create a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Split training dataset in `training` and `validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    test_size=0.33,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Create DecisionTree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model = dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Validation of the model\n",
    "\n",
    "### 1.3.1 - Validate with `train` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3977\n",
      "           1       1.00      1.00      1.00      4524\n",
      "           2       1.00      1.00      1.00      4017\n",
      "           3       1.00      1.00      1.00      4096\n",
      "           4       1.00      1.00      1.00      3924\n",
      "           5       1.00      1.00      1.00      3622\n",
      "           6       1.00      1.00      1.00      3938\n",
      "           7       1.00      1.00      1.00      4144\n",
      "           8       1.00      1.00      1.00      3957\n",
      "           9       1.00      1.00      1.00      4001\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     40200\n",
      "   macro avg       1.00      1.00      1.00     40200\n",
      "weighted avg       1.00      1.00      1.00     40200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "predict_result = []\n",
    "\n",
    "predictions = dt_model.predict(X_train)\n",
    "\n",
    "predict_result = zip(y_train, predictions)\n",
    "\n",
    "result_training = pd.DataFrame(predict_result, columns=[\"y\",\"ŷ\"])\n",
    "\n",
    "print(classification_report(result_training[\"y\"],result_training[\"ŷ\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 - Validate with `validation` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      1946\n",
      "           1       0.94      0.95      0.95      2218\n",
      "           2       0.84      0.84      0.84      1941\n",
      "           3       0.83      0.83      0.83      2035\n",
      "           4       0.84      0.87      0.86      1918\n",
      "           5       0.82      0.82      0.82      1799\n",
      "           6       0.90      0.89      0.90      1980\n",
      "           7       0.90      0.90      0.90      2121\n",
      "           8       0.78      0.78      0.78      1894\n",
      "           9       0.82      0.81      0.81      1948\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     19800\n",
      "   macro avg       0.86      0.86      0.86     19800\n",
      "weighted avg       0.86      0.86      0.86     19800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_result = []\n",
    "\n",
    "predictions = dt_model.predict(X_validate)\n",
    "\n",
    "predict_result = zip(y_validate, predictions)\n",
    "\n",
    "result_validate = pd.DataFrame(predict_result, columns=[\"y\",\"ŷ\"])\n",
    "\n",
    "print(classification_report(result_validate[\"y\"],result_validate[\"ŷ\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - CrossValidaion\n",
    " - Apply CrossValidation with 20 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "cv_result = cross_val_score(dt_model, list(train_images), list(train_labels), cv=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\n",
      "[0.87857618 0.87920133 0.86813187 0.88141239 0.84776815 0.85938021\n",
      " 0.88237254 0.86837721 0.86937687 0.87033333 0.858      0.87758506\n",
      " 0.86490994 0.86891261 0.87991995 0.86924616 0.86757839 0.89026017\n",
      " 0.88518024 0.88551402]\n",
      "Acurracy Mean: 0.872601831239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Accuracy:\n",
    "{}\n",
    "Acurracy Mean: {}\n",
    "\"\"\".format(cv_result, np.mean(cv_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model = dt_model.fit(train_images,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Test the classifier with the `test` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === RESULT === \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92       980\n",
      "           1       0.96      0.96      0.96      1135\n",
      "           2       0.88      0.84      0.86      1032\n",
      "           3       0.83      0.86      0.84      1010\n",
      "           4       0.87      0.88      0.88       982\n",
      "           5       0.82      0.84      0.83       892\n",
      "           6       0.90      0.88      0.89       958\n",
      "           7       0.91      0.90      0.90      1028\n",
      "           8       0.81      0.80      0.81       974\n",
      "           9       0.86      0.84      0.85      1009\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_result = []\n",
    "\n",
    "predictions = dt_model.predict(test_images)\n",
    "\n",
    "predict_result = zip(test_labels, predictions)\n",
    "\n",
    "result_validate = pd.DataFrame(predict_result, columns=[\"y\",\"ŷ\"])\n",
    "print(\" === RESULT === \")\n",
    "print(classification_report(result_validate[\"y\"],result_validate[\"ŷ\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
